{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUVCb952itzACfA3FqyMLy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DATAGEEKN/AI_SPRINT_2025_SmartVision_Counter_PaliGemma.ipynb/blob/main/AI_SPRINT_2025_SmartVision_Counter_PaliGemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYnR8JIouUZ4"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "print(\"--- Starting Hugging Face Authentication ---\")\n",
        "notebook_login()\n",
        "print(\"--- Hugging Face Authentication Widget Displayed. Please enter your token. ---\")\n",
        "print(\"After entering token and seeing 'Login successful', RESTART RUNTIME and then run the main code cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 1. Installations ---\n",
        "print(\"\\n--- 1. Performing Installations ---\")\n",
        "# Install necessary libraries. `kagglehub` is removed as it caused NameErrors and is not explicitly needed.\n",
        "!pip install -q transformers peft accelerate bitsandbytes torch torchvision pillow supervision\n",
        "print(\"Installations complete.\")"
      ],
      "metadata": {
        "id": "wXkKVyLIu6MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Imports ---\n",
        "print(\"\\n--- 2. Performing Imports ---\")\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import shutil\n",
        "import re\n",
        "import numpy as np # For dummy image generation\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from PIL import Image, ImageDraw # For drawing on dummy images\n",
        "import supervision as sv"
      ],
      "metadata": {
        "id": "PZbNhmj_uk5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. API Key & Device Setup ---\n",
        "print(\"\\n--- 3. Setting up API Keys and Device ---\")\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "kaggle_credentials_loaded = False\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "    os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "    if os.environ.get(\"KAGGLE_USERNAME\") and os.environ.get(\"KAGGLE_KEY\"):\n",
        "        kaggle_credentials_loaded = True\n",
        "        print(\"Kaggle API credentials loaded successfully from Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not load Kaggle credentials: {e}. Check Colab Secrets.\")\n",
        "\n",
        "# Roboflow API key no longer strictly needed for this version,  .\n",
        "roboflow_api_key_loaded = True # Assume true if not critical for this specific code path\n",
        "try:\n",
        "    if userdata.get('ROBOFLOW_API_KEY'):\n",
        "        os.environ[\"ROBOFLOW_API_KEY\"] = userdata.get('ROBOFLOW_API_KEY')\n",
        "        print(\"Roboflow API key found (though not critical for this dummy dataset version).\")\n",
        "except Exception: pass # Ignore if not present\n",
        "\n",
        "if not kaggle_credentials_loaded: # Only Kaggle critical now for model download\n",
        "    raise RuntimeError(\"CRITICAL: Kaggle API key is NOT loaded. Please fix in Colab Secrets and RESTART RUNTIME.\")\n",
        "\n",
        "os.makedirs(\"datasets\", exist_ok=True)\n",
        "os.makedirs(\"output_images\", exist_ok=True)\n",
        "print(\"API keys and device setup complete.\")\n"
      ],
      "metadata": {
        "id": "pJCK04O5uk7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 4. Load PaliGemma Model and Configure LoRA ---\n",
        "print(\"\\n--- 4. Loading PaliGemma Model and Configuring LoRA ---\")\n",
        "model_id = \"google/paligemma-3b-mix-224\"\n",
        "processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
        "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
        "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
        ")\n",
        "# Prepare for 4-bit quantization and LoRA fine-tuning\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "lora_config = LoraConfig(\n",
        "    r=8, # LoRA rank\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# --- Ensure LoRA adapters have requires_grad=True (Fix for grad_fn error) ---\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora\" in name:\n",
        "        param.requires_grad = True\n",
        "    elif \"norm\" in name: # Often beneficial to train layernorms\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False # Freeze base model parameters\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "print(\"PaliGemma model loaded and LoRA configured.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DiEoersruk-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Generate DUMMY Dataset ---\n",
        "print(\"\\n--- 5. Generating DUMMY Dataset ---\")\n",
        "\n",
        "DUMMY_DATASET_DIR = \"datasets/dummy_fruit_data\"\n",
        "os.makedirs(os.path.join(DUMMY_DATASET_DIR, 'train', 'images'), exist_ok=True)\n",
        "os.makedirs(os.path.join(DUMMY_DATASET_DIR, 'val', 'images'), exist_ok=True)\n",
        "os.makedirs(os.path.join(DUMMY_DATASET_DIR, 'test', 'images'), exist_ok=True)\n",
        "\n",
        "# Define dummy classes and some colors for visualization\n",
        "ALL_CLASSES = [\"apple\", \"banana\", \"orange\"]\n",
        "CLASS_COLORS = [(255, 0, 0), (255, 255, 0), (255, 165, 0)] # Red, Yellow, Orange\n",
        "\n",
        "def generate_dummy_image_and_annotation(image_id, dataset_type, classes, img_size=512, num_objects=3):\n",
        "    img_array = np.zeros((img_size, img_size, 3), dtype=np.uint8) + 50 # Dark background\n",
        "    img = Image.fromarray(img_array)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    annotations = []\n",
        "\n",
        "    # Generate random objects and draw them\n",
        "    for i in range(num_objects):\n",
        "        class_idx = np.random.randint(0, len(classes))\n",
        "        class_name = classes[class_idx]\n",
        "        color = CLASS_COLORS[class_idx]\n",
        "\n",
        "        # Random bounding box (ensure within bounds and minimum size)\n",
        "        x_min = np.random.randint(0, img_size - 50)\n",
        "        y_min = np.random.randint(0, img_size - 50)\n",
        "        x_max = np.random.randint(x_min + 20, min(img_size, x_min + 100))\n",
        "        y_max = np.random.randint(y_min + 20, min(img_size, y_min + 100))\n",
        "\n",
        "        draw.rectangle([x_min, y_min, x_max, y_max], fill=color, outline=(255,255,255))\n",
        "        draw.text((x_min + 5, y_min + 5), class_name, fill=(255,255,255))\n",
        "\n",
        "        # Convert to PaliGemma 0-1023 normalized coordinates (y1, x1, y2, x2)\n",
        "        y1_norm = int((y_min / img_size) * 1023)\n",
        "        x1_norm = int((x_min / img_size) * 1023)\n",
        "        y2_norm = int((y_max / img_size) * 1023)\n",
        "        x2_norm = int((x_max / img_size) * 1023)\n",
        "\n",
        "        loc_tokens = f\"<loc{y1_norm:04d}><loc{x1_norm:04d}><loc{y2_norm:04d}><loc{x2_norm:04d}>\"\n",
        "        annotations.append({\"loc_tokens\": loc_tokens, \"class_name\": class_name})\n",
        "\n",
        "    img_filename = f\"{dataset_type}_{image_id:03d}.jpg\"\n",
        "    img_save_path = os.path.join(DUMMY_DATASET_DIR, dataset_type, 'images', img_filename)\n",
        "    img.save(img_save_path)\n",
        "\n",
        "    # Prepare JSONL entry for PaliGemma format\n",
        "    annotations.sort(key=lambda x: x[\"class_name\"])\n",
        "    unique_classes_in_image = sorted(list(set(obj[\"class_name\"] for obj in annotations)))\n",
        "    prefix = \"detect \" + \" ; \".join(unique_classes_in_image)\n",
        "    suffix = \" ; \".join([f\"{obj['loc_tokens']} {obj['class_name']}\" for obj in annotations])\n",
        "\n",
        "    return {\n",
        "        \"image\": os.path.join(dataset_type, 'images', img_filename),\n",
        "        \"prefix\": prefix,\n",
        "        \"suffix\": suffix\n",
        "    }\n",
        "\n",
        "# Generate dummy data for train, val, test splits\n",
        "dummy_train_entries = []\n",
        "dummy_val_entries = []\n",
        "dummy_test_entries = []\n",
        "\n",
        "num_train_samples = 50\n",
        "num_val_samples = 10\n",
        "num_test_samples = 10\n",
        "\n",
        "for i in tqdm(range(num_train_samples), desc=\"Generating Train Data\"):\n",
        "    dummy_train_entries.append(generate_dummy_image_and_annotation(i, 'train', ALL_CLASSES))\n",
        "\n",
        "for i in tqdm(range(num_val_samples), desc=\"Generating Val Data\"):\n",
        "    dummy_val_entries.append(generate_dummy_image_and_annotation(i, 'val', ALL_CLASSES))\n",
        "\n",
        "for i in tqdm(range(num_test_samples), desc=\"Generating Test Data\"):\n",
        "    dummy_test_entries.append(generate_dummy_image_and_annotation(i, 'test', ALL_CLASSES))\n",
        "\n",
        "# Save dummy data as JSONL files\n",
        "TRAIN_JSONL_PATH = os.path.join(DUMMY_DATASET_DIR, '_annotations.train.jsonl')\n",
        "VAL_JSONL_PATH = os.path.join(DUMMY_DATASET_DIR, '_annotations.val.jsonl')\n",
        "TEST_JSONL_PATH = os.path.join(DUMMY_DATASET_DIR, '_annotations.test.jsonl')\n",
        "\n",
        "with open(TRAIN_JSONL_PATH, 'w') as f:\n",
        "    for entry in dummy_train_entries: f.write(json.dumps(entry) + '\\n')\n",
        "with open(VAL_JSONL_PATH, 'w') as f:\n",
        "    for entry in dummy_val_entries: f.write(json.dumps(entry) + '\\n')\n",
        "with open(TEST_JSONL_PATH, 'w') as f:\n",
        "    for entry in dummy_test_entries: f.write(json.dumps(entry) + '\\n')\n",
        "# Define paths consistent with dataset loaders\n",
        "TRAIN_IMAGES_DIR = os.path.join(DUMMY_DATASET_DIR, 'train', 'images')\n",
        "VAL_IMAGES_DIR = os.path.join(DUMMY_DATASET_DIR, 'val', 'images')\n",
        "TEST_IMAGES_DIR = os.path.join(DUMMY_DATASET_DIR, 'test', 'images')\n",
        "ROBOFLOW_DATA_ROOT = DUMMY_DATASET_DIR # Used as image_directory_root in dataset class\n",
        "ALL_CLASSES = ALL_CLASSES # Already defined above\n",
        "\n",
        "print(\"Dummy dataset generation complete.\")\n"
      ],
      "metadata": {
        "id": "2dlnqmcIvLnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Define Custom Dataset Class and Instantiate DataLoaders ---\n",
        "print(\"\\n--- 6. Initializing Custom Dataset Loaders ---\")\n",
        "\n",
        "class PaliGemmaODDataset(Dataset):\n",
        "    def __init__(self, jsonl_file_path: str, image_directory_root: str, processor):\n",
        "        self.entries = self._load_entries(jsonl_file_path)\n",
        "        self.image_directory_root = image_directory_root\n",
        "        self.processor = processor\n",
        "        self.max_length = 256\n",
        "    def _load_entries(self, file_path: str):\n",
        "        if not os.path.exists(file_path): print(f\"WARNING: JSONL file not found at {file_path}. Dataset will be empty.\"); return []\n",
        "        with open(file_path, 'r') as file: return [json.loads(line) for line in file]\n",
        "    def __len__(self): return len(self.entries)\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.entries[idx]\n",
        "        image_path = os.path.join(self.image_directory_root, entry['image'])\n",
        "        if not os.path.exists(image_path): print(f\"WARNING: Image file not found for entry {idx}: {image_path}. Skipping.\"); return self.__getitem__((idx + 1) % len(self.entries))\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # CORRECTED: Add <image> token to text_input as recommended by processor\n",
        "        text_input = \"<image>\" + entry['prefix'] + entry['suffix']\n",
        "\n",
        "        inputs = self.processor(\n",
        "            text=text_input,\n",
        "            images=image,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "        inputs['labels'] = inputs['input_ids'].clone()\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        return inputs\n",
        "\n",
        "DATASET_ROOT = DUMMY_DATASET_DIR # Now points to the dummy data\n",
        "train_dataset = PaliGemmaODDataset(TRAIN_JSONL_PATH, DATASET_ROOT, processor)\n",
        "val_dataset = PaliGemmaODDataset(VAL_JSONL_PATH, DATASET_ROOT, processor) if VAL_JSONL_PATH and os.path.exists(VAL_JSONL_PATH) else None\n",
        "test_dataset = PaliGemmaODDataset(TEST_JSONL_PATH, DATASET_ROOT, processor) if os.path.exists(TEST_JSONL_PATH) else None\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)} samples.\")\n",
        "if val_dataset: print(f\"Validation dataset size: {len(val_dataset)} samples.\")\n",
        "if test_dataset: print(f\"Test dataset size: {len(test_dataset)} samples.\")\n",
        "print(\"Custom Dataset Loaders Initialized.\")\n"
      ],
      "metadata": {
        "id": "70LpbRbOvLb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 7. Fine-tuning Loop ---\n",
        "print(\"\\n--- 7. Starting Fine-tuning Process ---\")\n",
        "output_dir_path = \"./paligemma_fine_tuned_fruit_counter\"\n",
        "os.makedirs(output_dir_path, exist_ok=True)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir_path, num_train_epochs=5,\n",
        "    per_device_train_batch_size=1, # Reduced for memory (1 image per GPU batch)\n",
        "    gradient_accumulation_steps=16, # Increased to compensate (effective batch size 1 * 16 = 16)\n",
        "    learning_rate=2e-5, fp16=True if torch.cuda.is_available() else False,\n",
        "    gradient_checkpointing=True, # Added for memory optimization (trades speed for memory)\n",
        "    logging_dir=\"./logs\", logging_steps=50,\n",
        "    save_steps=200, save_total_limit=2,\n",
        "    eval_strategy=\"steps\" if val_dataset else \"no\", # Corrected argument name\n",
        "    eval_steps=200 if val_dataset else None, load_best_model_at_end=True if val_dataset else False,\n",
        "    metric_for_best_model=\"eval_loss\" if val_dataset else None, greater_is_better=False, push_to_hub=False, report_to=\"tensorboard\"\n",
        ")\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, tokenizer=processor)\n",
        "\n",
        "if len(train_dataset) == 0:\n",
        "    print(\"ERROR: No training data available. Cannot start training.\")\n",
        "else:\n",
        "    trainer.train()\n",
        "    print(\"Fine-tuning training complete.\")\n",
        "    model.save_pretrained(output_dir_path)\n",
        "    processor.save_pretrained(output_dir_path)\n",
        "    print(f\"Fine-tuned model and processor saved to: {output_dir_path}\")\n",
        "print(\"Fine-tuning Process Complete.\")\n"
      ],
      "metadata": {
        "id": "Bze2QF0_vLZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Inference and Object Counting ---\n",
        "print(\"\\n--- 8. Starting Inference and Object Counting ---\")\n",
        "fine_tuned_model_path = \"./paligemma_fine_tuned_fruit_counter\"\n",
        "# Ensure the model (fine-tuned or base) is loaded correctly for inference\n",
        "if not os.path.exists(fine_tuned_model_path) or not os.path.exists(os.path.join(fine_tuned_model_path, 'config.json')):\n",
        "    print(f\"WARNING: Fine-tuned model not found at '{fine_tuned_model_path}'. Loading base model.\")\n",
        "    model_id = \"google/paligemma-3b-mix-224\"\n",
        "    # processor defined earlier\n",
        "    model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "else:\n",
        "    # processor defined earlier\n",
        "    processor = PaliGemmaProcessor.from_pretrained(fine_tuned_model_path)\n",
        "    model = PaliGemmaForConditionalGeneration.from_pretrained(fine_tuned_model_path, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "model.eval()\n",
        "\n",
        "if 'ALL_CLASSES' not in globals() or not ALL_CLASSES:\n",
        "    print(\"WARNING: ALL_CLASSES list is empty or not found. Defaulting to ['apple', 'banana', 'orange'].\")\n",
        "    ALL_CLASSES = [\"apple\", \"banana\", \"orange\"]\n",
        "print(f\"Classes for detection: {ALL_CLASSES}\")\n",
        "\n",
        "def predict_and_count(image_path: str, classes_to_detect: list, model, processor, device):\n",
        "    if not os.path.exists(image_path): print(f\"Error: Image not found at {image_path}.\"); return {}, None\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    prompt = f\"detect {'; '.join(classes_to_detect)}\"\n",
        "    # IMPORTANT: Ensure the <image> token is also prepended for inference\n",
        "    inputs = processor(text=\"<image>\" + prompt, images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad(): output_ids = model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
        "    generated_text = processor.decode(output_ids[0], skip_special_tokens=True)\n",
        "    clean_generated_text = generated_text.replace(prompt, '').strip() # Remove the prompt *without* <image> for clean parsing\n",
        "    clean_generated_text = clean_generated_text.replace(\"<image>\", \"\").strip() # Remove any remaining <image>\n",
        "\n",
        "    print(f\"\\n--- Processing Image: {os.path.basename(image_path)} ---\")\n",
        "    print(f\"Input Prompt: '{prompt}'\")\n",
        "    print(f\"Cleaned Detection String: '{clean_generated_text}'\")\n",
        "    detections = sv.Detections.empty()\n",
        "    try:\n",
        "        # Corrected for supervision >= 0.10.0\n",
        "        detections = sv.Detections.from_lmm(lmm='paligemma', result=clean_generated_text, resolution_wh=image.size, classes=classes_to_detect)\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Error parsing detections for {image_path} with supervision: {e}. Detections will be empty.\");\n",
        "    object_counts = {}\n",
        "    if detections.class_id is not None:\n",
        "        for class_id in detections.class_id:\n",
        "            if class_id < len(classes_to_detect): object_counts[classes_to_detect[class_id]] = object_counts.get(classes_to_detect[class_id], 0) + 1\n",
        "            else: print(f\"WARNING: Detected class_id {class_id} out of bounds. Skipping count.\");\n",
        "    print(\"\\nDetected Objects and Counts:\")\n",
        "    if not object_counts: print(\"No objects detected.\");\n",
        "    else:\n",
        "        for obj, count in object_counts.items(): print(f\"- {obj}: {count}\");\n",
        "\n",
        "    # Corrected for supervision >= 0.10.0 - Use BoxAnnotator and LabelAnnotator separately\n",
        "    box_annotator = sv.BoxAnnotator()\n",
        "    label_annotator = sv.LabelAnnotator() # New annotator for labels\n",
        "\n",
        "    # First, annotate the boxes\n",
        "    annotated_frame_np = box_annotator.annotate(scene=image.copy(), detections=detections)\n",
        "\n",
        "    #annotate the labels on the already-boxed frame\n",
        "    labels = [f\"{classes_to_detect[class_id]}\" for class_id in detections.class_id if class_id < len(classes_to_detect)]\n",
        "    annotated_frame_np = label_annotator.annotate(scene=annotated_frame_np, detections=detections, labels=labels)\n",
        "\n",
        "    annotated_frame_pil = Image.fromarray(annotated_frame_np)\n",
        "    output_image_filename = f\"annotated_{os.path.basename(image_path)}\"\n",
        "    output_image_path = os.path.join(\"output_images\", output_image_filename)\n",
        "    annotated_frame_pil.save(output_image_path)\n",
        "    print(f\"Annotated image saved to: {output_image_path}\")\n",
        "    return object_counts, output_image_path\n",
        "\n",
        "print(\"\\n--- Starting Inference Examples ---\")\n",
        "import random\n",
        "inference_image_paths = []\n",
        "num_inference_examples = 5\n",
        "\n",
        "if test_dataset and len(test_dataset) > 0:\n",
        "    print(f\"Selecting {min(num_inference_examples, len(test_dataset))} random images from TEST dataset.\")\n",
        "    sample_entries = random.sample(test_dataset.entries, min(num_inference_examples, len(test_dataset)))\n",
        "    inference_image_paths = [os.path.join(test_dataset.image_directory_root, entry['image']) for entry in sample_entries]\n",
        "elif train_dataset and len(train_dataset) > 0:\n",
        "    print(f\"WARNING: Test dataset not available/empty. Selecting {min(num_inference_examples, len(train_dataset))} random images from TRAIN dataset.\")\n",
        "    sample_entries = random.sample(train_dataset.entries, min(num_inference_examples, len(train_dataset)))\n",
        "    inference_image_paths = [os.path.join(train_dataset.image_directory_root, entry['image']) for entry in sample_entries]\n",
        "else:\n",
        "    print(\"ERROR: No training or test data found. Cannot run inference examples.\")\n",
        "\n",
        "for i, img_path in enumerate(inference_image_paths):\n",
        "    counts, _ = predict_and_count(img_path, ALL_CLASSES, model, processor, DEVICE)\n",
        "print(\"Inference and Object Counting Complete.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "h1i4K4T7vnYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V9_GL5_RvnUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4TThz02vnLl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}